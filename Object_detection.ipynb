{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Object detection.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPy8IFlgjvQ3o2BLNyPJJ7g",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mahikarimib/object-detection/blob/main/Object_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Object Detection - YOLO\n",
        "\n",
        "Tiny YOLO"
      ],
      "metadata": {
        "id": "Isi2Yd-7rWpK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VCqknOUnc8uO"
      },
      "outputs": [],
      "source": [
        "def non_max_suppress(conf, xy_min, xy_max, threshold=.4):\n",
        "    _, _, classes = conf.shape\n",
        "    boxes = [(_conf, _xy_min, _xy_max) for _conf, _xy_min, _xy_max in zip(conf.reshape(-1, classes), xy_min.reshape(-1, 2), xy_max.reshape(-1, 2))]\n",
        "\n",
        "    # Iterate each class\n",
        "    for c in range(classes):\n",
        "        # Sort boxes according to their prob for class c\n",
        "        boxes.sort(key=lambda box: box[0][c], reverse=True)\n",
        "        # Iterate each box\n",
        "        for i in range(len(boxes) - 1):\n",
        "            box_i = boxes[i]\n",
        "            if box_i[0][c] == 0:\n",
        "                continue\n",
        "            for box_j in boxes[i + 1:]:\n",
        "                # Take iou threshold into account\n",
        "                if iou(box_i[1], box_i[2], box_j[1], box_j[2]) >= threshold:\n",
        "                    box_j[0][c] = 0\n",
        "    return boxes"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Intersection over Union (IoU)"
      ],
      "metadata": {
        "id": "1fDn7Icae3GK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def iou(b1, b2):\n",
        "    # determine the coordinates of the intersection rectangle\n",
        "    # each box is a list of four numbers like [x1, y1, x2, y2]\n",
        "    xA = max(b1[0], b2[0])\n",
        "    yA = max(b1[1], b2[1])\n",
        "    xB = min(b1[2], b2[2])\n",
        "    yB = min(b1[3], b2[3])\n",
        "\n",
        "    # compute the area of intersection\n",
        "    area_intersect = (xB - xA + 1) * (yB - yA + 1)\n",
        "\n",
        "    # Calculate area of the two boxes\n",
        "    area_b1 = (b1[2] - b1[0] + 1) * (b1[3] - b1[1] + 1)\n",
        "    area_b2 = (b2[2] - b2[0] + 1) * (b2[3] - b2[1] + 1)\n",
        "\n",
        "    # compute and return the intersection over union\n",
        "    return area_intersect / float(area_b1 + area_b2 - area_intersect)"
      ],
      "metadata": {
        "id": "9WWc1uPGeGyk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Library"
      ],
      "metadata": {
        "id": "skLXqtyGfK6n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import warnings\n",
        "from glob import glob\n",
        "from PIL import Image\n",
        "from collections import OrderedDict\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from yolo_loss import YoloLoss\n",
        "from utils import *"
      ],
      "metadata": {
        "id": "QkLMzFczfJLb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tiny YOLO implementation"
      ],
      "metadata": {
        "id": "jjVmKkq2iV03"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.rand(1, 1, 3, 3)\n",
        "print(x.size(), x)"
      ],
      "metadata": {
        "id": "n2LUc40OfVyJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = F.pad(x, (0, 1, 0, 1), mode='replicate')\n",
        "print(x.size(), x)"
      ],
      "metadata": {
        "id": "dOrwWnkgipOW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = F.max_pool2d(x, 2, stride=1)\n",
        "print(x.size(), x)"
      ],
      "metadata": {
        "id": "QuNLzs2Ripcc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MaxPoolStride1(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MaxPoolStride1, self).__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.max_pool2d(F.pad(x, (0,1,0,1), mode='replicate'), 2, stride=1)\n",
        "        return x\n",
        "\n",
        "\n",
        "class TinyYoloNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TinyYoloNet, self).__init__()\n",
        "        \n",
        "        self.num_classes = 20  # VOC PASCAL\n",
        "        self.anchors = [1.08,1.19,  3.42,4.41,  6.63,11.38,  9.42,5.11,  16.62,10.52]\n",
        "        self.num_anchors = len(self.anchors) / 2\n",
        "        num_output = (5 + self.num_classes) * self.num_anchors\n",
        "        \n",
        "        # 7 x 7 feature map\n",
        "        self.width = 224  \n",
        "        self.height = 224\n",
        "        \n",
        "        # loss function\n",
        "        self.loss = YoloLoss(self.num_classes, self.anchors, self.num_anchors)\n",
        "        \n",
        "        # Convultional Neural Network\n",
        "        self.cnn = nn.Sequential(OrderedDict([\n",
        "            # conv1\n",
        "            ('conv1', nn.Conv2d(3, 16, 3, 1, 1, bias=False)),\n",
        "            ('bn1', nn.BatchNorm2d(16)),\n",
        "            ('leaky1', nn.LeakyReLU(0.1, inplace=True)),\n",
        "            ('pool1', nn.MaxPool2d(2, 2)),\n",
        "\n",
        "            # conv2\n",
        "            ('conv2', nn.Conv2d(16, 32, 3, 1, 1, bias=False)),\n",
        "            ('bn2', nn.BatchNorm2d(32)),\n",
        "            ('leaky2', nn.LeakyReLU(0.1, inplace=True)),\n",
        "            ('pool2', nn.MaxPool2d(2, 2)),\n",
        "\n",
        "            # conv3\n",
        "            ('conv3', nn.Conv2d(32, 64, 3, 1, 1, bias=False)),\n",
        "            ('bn3', nn.BatchNorm2d(64)),\n",
        "            ('leaky3', nn.LeakyReLU(0.1, inplace=True)),\n",
        "            ('pool3', nn.MaxPool2d(2, 2)),\n",
        "\n",
        "            # conv4\n",
        "            ('conv4', nn.Conv2d(64, 128, 3, 1, 1, bias=False)),\n",
        "            ('bn4', nn.BatchNorm2d(128)),\n",
        "            ('leaky4', nn.LeakyReLU(0.1, inplace=True)),\n",
        "            ('pool4', nn.MaxPool2d(2, 2)),\n",
        "\n",
        "            # conv5\n",
        "            ('conv5', nn.Conv2d(128, 256, 3, 1, 1, bias=False)),\n",
        "            ('bn5', nn.BatchNorm2d(256)),\n",
        "            ('leaky5', nn.LeakyReLU(0.1, inplace=True)),\n",
        "            ('pool5', nn.MaxPool2d(2, 2)),\n",
        "\n",
        "            # conv6\n",
        "            ('conv6', nn.Conv2d(256, 512, 3, 1, 1, bias=False)),\n",
        "            ('bn6', nn.BatchNorm2d(512)),\n",
        "            ('leaky6', nn.LeakyReLU(0.1, inplace=True)),\n",
        "            ('pool6', MaxPoolStride1()),   # does not change tensor height and width\n",
        "\n",
        "            # conv7\n",
        "            ('conv7', nn.Conv2d(512, 1024, 3, 1, 1, bias=False)),\n",
        "            ('bn7', nn.BatchNorm2d(1024)),\n",
        "            ('leaky7', nn.LeakyReLU(0.1, inplace=True)),\n",
        "\n",
        "            # conv8\n",
        "            ('conv8', nn.Conv2d(1024, 1024, 3, 1, 1, bias=False)),\n",
        "            ('bn8', nn.BatchNorm2d(1024)),\n",
        "            ('leaky8', nn.LeakyReLU(0.1, inplace=True)),\n",
        "\n",
        "            # output: 125 = 5 * (4 + 1 + 20)\n",
        "            ('output', nn.Conv2d(1024, 125, 1, 1, 0))\n",
        "        ]))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.cnn(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "_kD2b0pXiphm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model(weights):\n",
        "    model = TinyYoloNet()\n",
        "    model.load_state_dict(torch.load(weights))"
      ],
      "metadata": {
        "id": "s3OfNBcYipmo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = load_model(weights='weights/tiny-yolo-voc.pth')"
      ],
      "metadata": {
        "id": "e04uYAfSipqD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = load_class_names('data/voc.names')\n",
        "print('  '.join(class_names))"
      ],
      "metadata": {
        "id": "BpCp2XFIiptF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(model, img_path, img_size=416, save_to=None, conf_thresh=0.4, nms_thresh=0.5):\n",
        "    # read image\n",
        "    img = Image.open(img_path).convert('RGB')\n",
        "    \n",
        "    # perform object detection\n",
        "    tic = time.time()\n",
        "    boxes = detect(model, img.resize((img_size, img_size)), conf_thresh, nms_thresh)\n",
        "    toc = time.time()\n",
        "    \n",
        "    # show results\n",
        "    print(\"Prediction time: [{:.5f} ms.]\".format((toc - tic) * 1000))\n",
        "    pred_img = plot_boxes(img, boxes, save_to, class_names) \n",
        "    \n",
        "    # return the image with predictions\n",
        "    return pred_img"
      ],
      "metadata": {
        "id": "LMIArpRMjiu7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict(model, 'data/eagle.jpg')"
      ],
      "metadata": {
        "id": "-Thmuz-RjjKy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict(model, 'data/giraffe.jpg')"
      ],
      "metadata": {
        "id": "vd06D7wHjjfO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "YOLO 2"
      ],
      "metadata": {
        "id": "PSRKAmq9kIwI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from yolo_v2 import YoloNet"
      ],
      "metadata": {
        "id": "NmzQGxj3jk8Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_yolo2(num_classes, anchors, weights):\n",
        "    yolo = YoloNet(num_classes, anchors)\n",
        "    yolo.load_state_dict(torch.load(weights))"
      ],
      "metadata": {
        "id": "rEVzWNXTjlU7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weight_file = 'weights/yolo.pth'\n",
        "\n",
        "# anchor and classes information\n",
        "class_names = open('data/coco.names').read().split('\\n')\n",
        "num_classes = len(class_names)\n",
        "\n",
        "anchors = [0.57273, 0.677385, 1.87446, 2.06253, 3.33843, 5.47434, 7.88282, 3.52778, 9.77052, 9.16828]\n",
        "\n",
        "image_size = 608"
      ],
      "metadata": {
        "id": "Ur8n9YCHjlv5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = load_yolo2(num_classes, anchors, weight_file)"
      ],
      "metadata": {
        "id": "Hlyu5OGbjj14"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict(model, 'data/dog.jpg', conf_thresh=0.3)"
      ],
      "metadata": {
        "id": "Tr3oEJ1Skfay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict(model, 'data/person.jpg')"
      ],
      "metadata": {
        "id": "-ZrAB1l5kg7q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}